{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_VIDEO = 0\n",
    "TRACK_TEAMS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "ModelDependencyMissing: Your `inference` configuration does not support PaliGemma model. Use pip install 'inference[transformers]' to install missing requirements.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support Florence2 model. Use pip install 'inference[transformers]' to install missing requirements.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support Qwen2.5-VL model. Use pip install 'inference[transformers]' to install missing requirements.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support SAM model. Use pip install 'inference[sam]' to install missing requirements.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support SAM model. Use pip install 'inference[sam]' to install missing requirements.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support SAM model. Use pip install 'inference[clip]' to install missing requirements.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support Gaze Detection model. Use pip install 'inference[gaze]' to install missing requirements.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support SmolVLM2.Use pip install 'inference[transformers]' to install missing requirements.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support GroundingDINO model. Use pip install 'inference[grounding-dino]' to install missing requirements.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support YoloWorld model. Use pip install 'inference[yolo-world]' to install missing requirements.\n"
     ]
    }
   ],
   "source": [
    "from inference import get_model\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sports.common.team import TeamClassifier\n",
    "from sports.annotators.soccer import draw_pitch, draw_points_on_pitch\n",
    "from sports.configs.soccer import SoccerPitchConfiguration\n",
    "from sports.common.view import ViewTransformer\n",
    "from supervision.draw.utils import draw_image\n",
    "from typing import List, Union\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"./../env/keys.env\")\n",
    "\n",
    "# Access the variables\n",
    "ROBOFLOW_API_KEY = os.getenv(\"ROBOFLOW_API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"ONNXRUNTIME_EXECUTION_PROVIDERS\"] = \"[CUDAExecutionProvider]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Your Model\n",
    "\n",
    "You can either download Roboflow's pretrained models or run your models that you've trained yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run these two cells to use Roboflow's Player & Detection Model** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'CoreMLExecutionProvider, AzureExecutionProvider, CPUExecutionProvider'\n",
      "UserWarning: Specified provider 'OpenVINOExecutionProvider' is not in available provider names.Available providers: 'CoreMLExecutionProvider, AzureExecutionProvider, CPUExecutionProvider'\n"
     ]
    }
   ],
   "source": [
    "PLAYER_DETECTION_MODEL_ID = \"football-players-detection-3zvbc/11\"\n",
    "PLAYER_DETECTION_MODEL = get_model(model_id=PLAYER_DETECTION_MODEL_ID, api_key=ROBOFLOW_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIELD_DETECTION_MODEL_ID = \"football-field-detection-f07vi/14\"\n",
    "FIELD_DETECTION_MODEL = get_model(model_id=FIELD_DETECTION_MODEL_ID, api_key=ROBOFLOW_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run these two cells to a local Player & Detection Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAYER_MODEL_FILE_NAME = 'best.pt'\n",
    "PLAYER_MODEL_PATH = \"./../models/\" + PLAYER_MODEL_FILE_NAME\n",
    "PLAYER_DETECTION_MODEL = YOLO(PLAYER_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIELD_MODEL_FILE_NAME = 'best.pt'\n",
    "FIELD_MODEL_PATH = \"./../models/\" + FIELD_MODEL_FILE_NAME\n",
    "FIELD_DETECTION_MODEL = YOLO(FIELD_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update `VIDEO_file` with the match footage you want to track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_FILE = \"2e57b9_0.mp4\"\n",
    "SOURCE_VIDEO_PATH = './footage/' + VIDEO_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = {\n",
    "    \"ball\" : 0,\n",
    "    \"goalkeeper\" : 1,\n",
    "    \"player\" : 2,\n",
    "    \"referee\" :3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Annotations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ellipse_annotator = sv.EllipseAnnotator(\n",
    "    color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),\n",
    "    thickness=2\n",
    ")\n",
    "label_annotator = sv.LabelAnnotator(\n",
    "    color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),\n",
    "    text_color=sv.Color.from_hex('#000000'),\n",
    "    text_position=sv.Position.BOTTOM_CENTER\n",
    ")\n",
    "triangle_annotator = sv.TriangleAnnotator(\n",
    "    color=sv.Color.from_hex('#FFD700'),\n",
    "    base=25,\n",
    "    height=21,\n",
    "    outline_thickness=1\n",
    ")\n",
    "box_annotator = sv.BoxAnnotator(\n",
    "    color=sv.ColorPalette.from_hex(['#FF8C00', '#00BFFF', '#FF1493', '#FFD700']),\n",
    "    thickness=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identify Goal Keeper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_goalkeepers_team_id(players, goalkeepers):\n",
    "    goalkeepers_xy = goalkeepers.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)\n",
    "    players_xy = players.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)\n",
    "    team_0_centroid = players_xy[players.class_id == 0].mean(axis=0)\n",
    "    team_1_centroid = players_xy[players.class_id == 1].mean(axis=0)\n",
    "    goalkeepers_team_id = []\n",
    "    for goalkeeper_xy in goalkeepers_xy:\n",
    "        dist_0 = np.linalg.norm(goalkeeper_xy - team_0_centroid)\n",
    "        dist_1 = np.linalg.norm(goalkeeper_xy - team_1_centroid)\n",
    "        goalkeepers_team_id.append(0 if dist_0 < dist_1 else 1)\n",
    "\n",
    "    return np.array(goalkeepers_team_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Detections**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detections(frame, detections, key_points, tracker, team_classifier):\n",
    "  CONFIG = SoccerPitchConfiguration()\n",
    "\n",
    "  # Organize Detections\n",
    "  ball_detections = detections[detections.class_id == objects[\"ball\"]]\n",
    "  ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)\n",
    "\n",
    "  all_detections = detections[detections.class_id != objects[\"ball\"]]\n",
    "  all_detections = all_detections.with_nms(threshold=0.5, class_agnostic=True)\n",
    "  all_detections = tracker.update_with_detections(detections=all_detections)\n",
    "\n",
    "  goalkeepers_detections = all_detections[all_detections.class_id == objects[\"goalkeeper\"]]\n",
    "  players_detections = all_detections[all_detections.class_id == objects[\"player\"]]\n",
    "\n",
    "  if(team_classifier):\n",
    "    players_crops = [sv.crop_image(frame, xyxy) for xyxy in players_detections.xyxy]\n",
    "    players_detections.class_id = team_classifier.predict(players_crops)\n",
    "\n",
    "  goalkeepers_detections.class_id = resolve_goalkeepers_team_id(\n",
    "      players_detections, goalkeepers_detections)\n",
    "\n",
    "  # Adjust Points to 2D Pitch\n",
    "  filter = key_points.confidence[0] > 0.5\n",
    "  frame_reference_points = key_points.xy[0][filter]\n",
    "  pitch_reference_points = np.array(CONFIG.vertices)[filter]\n",
    "\n",
    "  transformer = ViewTransformer(\n",
    "      source=frame_reference_points,\n",
    "      target=pitch_reference_points\n",
    "  )\n",
    "\n",
    "  frame_ball_xy = ball_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)\n",
    "  pitch_ball_xy = transformer.transform_points(points=frame_ball_xy)\n",
    "  ball_detections.data[\"pitch_xy\"] = pitch_ball_xy\n",
    "\n",
    "  frame_goalkeepers_xy = goalkeepers_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)\n",
    "  pitch_goalkeepers_xy = transformer.transform_points(points=frame_goalkeepers_xy)\n",
    "  goalkeepers_detections.data[\"pitch_xy\"] = pitch_goalkeepers_xy\n",
    "\n",
    "  frame_players_xy = players_detections.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)\n",
    "  pitch_players_xy = transformer.transform_points(points=frame_players_xy)\n",
    "  players_detections.data[\"pitch_xy\"] = pitch_players_xy\n",
    "\n",
    "  # Merge Detections\n",
    "  all_detections = sv.Detections.merge([ players_detections, goalkeepers_detections ])\n",
    "\n",
    "  return (all_detections, ball_detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identify Teams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_team_model(video, PLAYER_DETECTION_MODEL):\n",
    "  STRIDE = 30\n",
    "\n",
    "  frame_generator = sv.get_video_frames_generator(\n",
    "      source_path=SOURCE_VIDEO_PATH, stride=STRIDE)\n",
    "\n",
    "  crops = []\n",
    "  for frame in tqdm(frame_generator, desc='collecting crops'):\n",
    "      result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]\n",
    "      detections = sv.Detections.from_inference(result)\n",
    "      players_crops = [sv.crop_image(frame, xyxy) for xyxy in detections.xyxy]\n",
    "      crops += players_crops\n",
    "\n",
    "  team_classifier = TeamClassifier(device=\"cuda\")\n",
    "  team_classifier.fit(crops)\n",
    "\n",
    "  return team_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tracking_results(players, ball, frames):\n",
    "  csv = \"Frame,Object,Object ID,Team,X1,Y1,X1,X2,X_Pitch,Y_Pitch\\n\"\n",
    "  for frame in range(1, frames):\n",
    "    for player in players:\n",
    "      player_data = players[player]\n",
    "      if str(frame) in player_data:\n",
    "        csv += str(frame) + \",player,\" + str(player) + \",\" + str(player_data[str(frame)][\"Team\"]) + \",\" + str(player_data[str(frame)][\"X1\"]) + \",\" + str(player_data[str(frame)][\"Y1\"]) + \",\"  + str(player_data[str(frame)][\"X2\"]) + \",\" + str(player_data[str(frame)][\"Y2\"]) + \",\" + str(player_data[str(frame)][\"X_Pitch\"]) + \",\" + str(player_data[str(frame)][\"Y_Pitch\"]) + \"\\n\"\n",
    "    if str(frame) in ball:\n",
    "      csv += str(frame) + \",ball,,,\" + str(ball[str(frame)][\"X1\"]) + \",\" + str(ball[str(frame)][\"Y1\"]) + \",\" + str(ball[str(frame)][\"X2\"]) + \",\" + str(ball[str(frame)][\"Y2\"]) + \",\" + str(ball[str(frame)][\"X_Pitch\"]) + \",\" + str(ball[str(frame)][\"Y_Pitch\"]) + \"\\n\"\n",
    "\n",
    "  with open(\"./output/\" + VIDEO_FILE + \".csv\", \"w\") as file:\n",
    "    file.write(csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tracking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = SoccerPitchConfiguration()\n",
    "tracking_results = \"\"\n",
    "\n",
    "#Get Video\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "\n",
    "tracker = sv.ByteTrack()\n",
    "tracker.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting crops: 25it [00:07,  3.13it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m team_classifier = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m(track_teams):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m   team_classifier = \u001b[43mgenerate_team_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSOURCE_VIDEO_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPLAYER_DETECTION_MODEL\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mgenerate_team_model\u001b[39m\u001b[34m(video, PLAYER_DETECTION_MODEL)\u001b[39m\n\u001b[32m     11\u001b[39m     players_crops = [sv.crop_image(frame, xyxy) \u001b[38;5;28;01mfor\u001b[39;00m xyxy \u001b[38;5;129;01min\u001b[39;00m detections.xyxy]\n\u001b[32m     12\u001b[39m     crops += players_crops\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m team_classifier = \u001b[43mTeamClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m team_classifier.fit(crops)\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m team_classifier\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/soccer_vision/lib/python3.11/site-packages/sports/common/team.py:57\u001b[39m, in \u001b[36mTeamClassifier.__init__\u001b[39m\u001b[34m(self, device, batch_size)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28mself\u001b[39m.device = device\n\u001b[32m     55\u001b[39m \u001b[38;5;28mself\u001b[39m.batch_size = batch_size\n\u001b[32m     56\u001b[39m \u001b[38;5;28mself\u001b[39m.features_model = \u001b[43mSiglipVisionModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mSIGLIP_MODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28mself\u001b[39m.processor = AutoProcessor.from_pretrained(SIGLIP_MODEL_PATH)\n\u001b[32m     59\u001b[39m \u001b[38;5;28mself\u001b[39m.reducer = umap.UMAP(n_components=\u001b[32m3\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/soccer_vision/lib/python3.11/site-packages/transformers/modeling_utils.py:3110\u001b[39m, in \u001b[36mPreTrainedModel.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3105\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[32m   3106\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3107\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3108\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3109\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m3110\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/soccer_vision/lib/python3.11/site-packages/torch/nn/modules/module.py:1343\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1340\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1341\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/soccer_vision/lib/python3.11/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/soccer_vision/lib/python3.11/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/soccer_vision/lib/python3.11/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/soccer_vision/lib/python3.11/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    927\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    928\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    931\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    933\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/soccer_vision/lib/python3.11/site-packages/torch/nn/modules/module.py:1329\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1323\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1324\u001b[39m             device,\n\u001b[32m   1325\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1326\u001b[39m             non_blocking,\n\u001b[32m   1327\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1328\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1335\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/soccer_vision/lib/python3.11/site-packages/torch/cuda/__init__.py:310\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    306\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    307\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmultiprocessing, you must use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mspawn\u001b[39m\u001b[33m'\u001b[39m\u001b[33m start method\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    308\u001b[39m     )\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch._C, \u001b[33m\"\u001b[39m\u001b[33m_cuda_getDeviceCount\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTorch not compiled with CUDA enabled\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    313\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    314\u001b[39m     )\n",
      "\u001b[31mAssertionError\u001b[39m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "team_classifier = None\n",
    "if(TRACK_TEAMS):\n",
    "  team_classifier = generate_team_model(SOURCE_VIDEO_PATH, PLAYER_DETECTION_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate Over Each Frame\n",
    "frame_number = 1\n",
    "video_info = sv.VideoInfo.from_video_path(video_path=SOURCE_VIDEO_PATH)\n",
    "players = {}\n",
    "ball = {}\n",
    "\n",
    "with sv.VideoSink(target_path=\"./output.mp4\", video_info=video_info) as sink:\n",
    "  for frame in tqdm(frame_generator, desc='Collecting Tracking Data...'):\n",
    "    result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]\n",
    "    detections = sv.Detections.from_inference(result)\n",
    "\n",
    "    result = FIELD_DETECTION_MODEL.infer(frame, confidence=0.3)[0]\n",
    "    key_points = sv.KeyPoints.from_inference(result)\n",
    "\n",
    "    #Organize Detections\n",
    "    all_detections, ball_detections = get_detections(frame, detections, key_points, tracker, team_classifier)\n",
    "    object_ids = all_detections.tracker_id\n",
    "    team_ids = all_detections.class_id\n",
    "    object_types = all_detections.data[\"class_name\"]\n",
    "    pitch_xys = all_detections.data[\"pitch_xy\"]\n",
    "    ball_pitch_xys = ball_detections.data[\"pitch_xy\"]\n",
    "    all_detections.class_id = all_detections.class_id.astype(int)\n",
    "\n",
    "    labels = [\n",
    "        f\"#{tracker_id}\"\n",
    "        for tracker_id\n",
    "        in all_detections.tracker_id\n",
    "    ]\n",
    "\n",
    "    #Iterate Over Frames\n",
    "    for idx, xyxy in enumerate(all_detections.xyxy):\n",
    "      team_id = 0\n",
    "      if(tracker):\n",
    "        team_id = team_ids[idx]\n",
    "\n",
    "      object_id = str(object_ids[idx])\n",
    "      if(object_id not in players):\n",
    "        players[object_id] = {}\n",
    "\n",
    "      players[object_id][str(frame_number)] = {\n",
    "          \"Object Type\" : object_types[idx],\n",
    "          \"Team\" : team_id,\n",
    "          \"X1\" : xyxy[0],\n",
    "          \"Y1\" : xyxy[1],\n",
    "          \"X2\" : xyxy[2],\n",
    "          \"Y2\" : xyxy[3],\n",
    "          \"X_Pitch\" : pitch_xys[idx][0],\n",
    "          \"Y_Pitch\" : pitch_xys[idx][1],\n",
    "          \"Y_MPLSoccer\" : float(float(pitch_xys[idx][1]) / float(CONFIG.width)),\n",
    "          \"X_MPLSoccer\" : float(float(pitch_xys[idx][0]) / float(CONFIG.length))\n",
    "      }\n",
    "\n",
    "    if(ball_detections.xyxy.shape[0]):\n",
    "      ball[str(frame_number)] = {\n",
    "            \"X1\" : ball_detections.xyxy[0][0],\n",
    "            \"Y1\" : ball_detections.xyxy[0][1],\n",
    "            \"X2\" : ball_detections.xyxy[0][2],\n",
    "            \"Y2\" : ball_detections.xyxy[0][3],\n",
    "            \"X_Pitch\" : ball_pitch_xys[0][0],\n",
    "            \"Y_Pitch\" : ball_pitch_xys[0][1],\n",
    "            \"Y_MPLSoccer\" : float(float(ball_pitch_xys[0][1]) / float(CONFIG.width)),\n",
    "            \"X_MPLSoccer\" : float(float(ball_pitch_xys[0][0]) / float(CONFIG.length))\n",
    "      }\n",
    "    else:\n",
    "      ball[str(frame_number)] = {\n",
    "            \"X1\" : 0,\n",
    "            \"Y1\" : 0,\n",
    "            \"X2\" : 0,\n",
    "            \"Y2\" : 0,\n",
    "            \"X_Pitch\" : 0,\n",
    "            \"Y_Pitch\" : 0,\n",
    "            \"Y_MPLSoccer\" : 0,\n",
    "            \"X_MPLSoccer\" : 0\n",
    "      }\n",
    "\n",
    "    frame_number += 1\n",
    "\n",
    "    if(GENERATE_VIDEO):\n",
    "      annotated_frame = frame.copy()\n",
    "      annotated_frame = ellipse_annotator.annotate(\n",
    "          scene=annotated_frame,\n",
    "          detections=all_detections)\n",
    "      annotated_frame = label_annotator.annotate(\n",
    "          scene=annotated_frame,\n",
    "          detections=all_detections,\n",
    "          labels=labels)\n",
    "      annotated_frame = triangle_annotator.annotate(\n",
    "          scene=annotated_frame,\n",
    "          detections=ball_detections)\n",
    "\n",
    "      sink.write_frame(frame=annotated_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_tracking_results(players, ball, frame_number)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soccer_vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
